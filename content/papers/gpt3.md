---
title: "Review - GPT-3"
date: 2021-05-19T17:05:46+03:00
draft: true
---

최근 연구는 큰 말뭉치에 대한 사전 훈련 방식과 미세 조정(fine tuning) 방식을 통해 상당한 성능 개선을 이뤘다. 언어 모델을 확장하면 작업 영역이 어떤 영역이든 간에 성능 개선이 이뤄지고 미세 조정의 접근으로 최신 성능에 근접할 수 있다는 것을 확인할 수 있다.



### GPT-3의 특징

GPT-3는 인간다운 자연어를 생성하기 위한 자기회귀 언어 모델이다. 기존 GPT-2에 비해 2배 이상 큰 언어모델이다.

- 1750억개 파라메터로 학습된 회귀 모델을 학습
- 그레디언트 업데이트나 미세 조정 없이 적용
- 번역과 QA와 같은 분야와 관련한 데이터셋에 대한 성능이 훌륭함
- 랜덤 글짓기, 간단한 사칙연산, 번역, 언어 문제 풀이, 기사 생성과 같은 문장 생성 작업에 활용 가능



### GPT-3의 한계 

GPT-3는 방대한 데이터를 통해 다음 단어가 무엇인지, 어떤 단어가 잘 어울리는지를 예측할 수 있다. 사람은 이해를 바탕으로 다음 단어에 대한 표현을 생성하는 반면, GPT-3는 통계적으로 가장 잘 어울리는 단어를 생성하는 관점이다. 이로 인해 다음과 같은 한계가 존재한다.

- 일반적인 상식에 대한 처리가 부족
- 일부 분야에서는 부족한 성능을 나타냄 (예) 복합 연산
- 입력이 적을때 성능이 떨어지는 특성
- 새로운 것에 대한 동기화 처리의 어려움이 있음



### 참고

https://arxiv.org/pdf/2005.14165.pdf



