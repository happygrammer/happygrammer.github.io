---
title: "트랜스포머 모델의 개발"
date: 2021-09-05T05:28:46+03:00
draft: false
---

트랜서포머는 어텐션 매커니즘이 적용된 딥러닝 모델이다. 2017년 구글이 발표한 "Attention is all you need" 논문에서 처음 소개 되었다. RNN(1986) 모델과 LSTM(1997)와 같은 Recurrent 모델을 대체할 수 있는 대안 모델이 되었다. 트랜스포머는 자연어 처리, 컴퓨터 비전, 오디오 처리와 갈은 분야에서 성공을 거뒀다. 그래서 관심을 받고 있다. 최근에는 트랜스포머의 다양한 변형이 존재한다.



### 트랜스포머 모델 개발

트랜서포머 모델을 개발하는 세가지 관점이 존재한다.

- 모델 효율성 : 계산과 메모리 복잡도를 개선하는 것이다. 이를 위해 셀프 어텐션 모듈을 이용한다.
- 모델 일반화 : 소규모 데이터에 학습이 쉽지 않다. 그래서  사전 훈련 방법, 레이블링 없는 대규모 데이터에 대해 학습을 한다.
- 모델 적용 : 하위 도메인(downstream)에 적용하기 위해 트랜스포머를 적용



### 트랜스포머 모델 배경

-  Vanilla Transformer
 - 인코더와 디코더로 이뤄진 시퀀스 투 시퀀스 모델. 각 인코더 블록은 멀티 헤드 셀프 어텐션 모듈(multi-head self-attention module)과 포지션-와이즈 피드 포워드 네트워크(FFN:position-wise feed-forward network)로 구성되어 있다.
 - 어텐션 모듈
 - 트랜스포머의 적용 메커니즘은 쿼리-키-밸류(QKV) 모델이다.

   ![](../transformers_1.png)
 - 트랜스포머는 싱글 어텐션 함수(single attention function) 대신에 멀티 헤드 어텐션(multi-head attention)을 이용한다.
 - `모델 사용 관점`에서, 트랜스포머 아키텍처는 세 가지 관점에서 사용된다.
    - 기계번역에서 시퀀스 투 시퀀스 모델링(sequence-to-sequence modeling) 사용된다.
    - 인코더만 사용 : 입력 시퀀스를 출력 용으로 사용할 수 있다. 분류 또는 시퀀스 라벨링 문제에 사용된다.
    - 디코더만 사용 : 언어 모델링의 시퀀스 생성에 사용된다.



### 모델 복잡도

트랜스포머 모델은 모델 계산 시간과 파라메터 개수로 모델의 성능을 설명할 수 있다. 트랜스 포머의 중요한 두가지 요소는 셀프 어텐션(self-attention) 모듈과포지션 와이즈 FFN(position-wise FFN) 모듈이다. 입력 시퀀스가 짧으면, 위 두 요소의 은닉 차원(hidden dimension)의 복잡도가 올라간다. 반대로  입력 시퀀스가 길어질수록 FFN 병목 현상이 발생할 수 있다.



### 네트워크 타입

셀프 어텐션 분석 : 셀프 어텐션은 입력의 길이가 가변적일 때 다룰수 있는 가변적인 매커니즘이다. 세 가지 유형이 있다.

- 셀프 어텐션
- 풀리 커넥티드(fully connected)
- 컨볼루셔널(convolutional)
- 리커런트(reucrrent)

- 셀프 어텐션의 이점은 다음과 같다.
  - 장거리 의존 모델링(long-range dependencies modeling)에 적합, 최대 길이가 동일한 풀리 커넥티드 레이어를 이용
  - 컨볼루셔널 레이어의 제한적인 수용이 있어 딥 네트워크를 쌓아야한다, 반면 최대 경로 길이가 일정한 경우, 레이어 개수를 고정하는 방식으로 장거리 의존성(long-range dependencies)에 대한 셀프 어텐션을 적용할 수 있다.
  - 연속 연산(constant sequential operations)과 최대 경로 길이(maximum path length)가 있으면 셀프 어텐션(self-attention)이 리커런트 레이어(recurrent layer)보다 장거리 모델링에 더 유리함



### 모델을 보는 네가지 관점

다양한 트랜스 포머가 있지만 여기서는 바닐라 트랜스 포머를 기초로 한다. 모델을 보는 네가지 관점이 존재한다.

- 모델 관점(Model) : 어테션, 포지셔닝 인코딩, LayerNorm, FFN
- 아키텍처 레벨 관점(Architecture Level) : Light Weight, Connectivity, 분할정복
- 사전 학습 방법(Pre-train) : 인코더(BERT, RoBERTA, BigBIRD), 디코더(GPT~), 인코더+디코더(BART, Switch Transformers)
- 응용 관점(applicaton) : NLP, CV, Audio, Multi Media

트랜스포머에서 어텐션 모듈이 가장 중요한 요소이다. 사전 훈련 변형과 관련한 아키텍처 변형이 존재한다. 그리고 이를 응용한 여러가지 방식들이 존재한다.



### 어텐션의 약점

셀프 어텐션(self-attention)은 트랜스 포머에서 중요한 역할이다. 다음과 같은 약점이 있다.

- 복잡도 : 긴 시퀀스를 다루는 것이 어렵다.
- 오버피팅 : 데이터양이 적으면 오버피팅이 되기 쉽다.

위의 약점에 대응하기 위해 몇가지 방식이 존재한다.

- 스파스 어텐션(sparse attention) : 스파스 바이어스를 어텐션 매커니즘에 반영하여 복잡도를 감소시킴
- 리니어라이즈드 어테션(linearlized attention) : 커널 피쳐 맵으로 어탠션 매트릭스를 분리함







